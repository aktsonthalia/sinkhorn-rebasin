<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>Re-basin via implicit Sinkhorn differentiation</title>
	<meta property="og:image" content="" />
	<meta property="og:title" content="Re-basin via implicit Sinkhorn differentiation" />
</head>

<body>
	<br>
	<center>
		<span style="font-size:42px">Re-basin via implicit Sinkhorn differentiation</span>
		<table align=center width=600px>
			<tr>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://www.linkedin.com/in/fidel-guerrero-pena/">Fidel A.
								G. Pena</a></span>
					</center>
				</td>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://heitorrapela.github.io/">Heitor
								Medeiros</a></span>
					</center>
				</td>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://github.com/ThomasDubail">Thomas Dubail</a></span>
					</center>
				</td>

		</table>

		<table align=center width=600px>
			<tr>
				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a href="https://github.com/Masseeh">Masih Aminbeidokhti</a></span>
					</center>
				</td>

				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a
								href="https://www.etsmtl.ca/en/research/professors/egranger">Eric Granger</a></span>
					</center>
				</td>

				<td align=center width=120px>
					<center>
						<span style="font-size:24px"><a
								href="https://www.etsmtl.ca/en/research/professors/mpedersoli/">Marco
								Pedersoli</a></span>
					</center>
				</td>
		</table>
		<span style="font-size:30px">CVPR 2023 </span>

		<table align=center width=650px>
			<tr>
				<!-- <td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href=''> [Demo]</a>
		  		  		</center>
		  		  	  </td> -->
				<td align=center width=150px>
					<center>
						<span style="font-size:24px"><a href='https://github.com/fagp/sinkhorn-rebasin'>
								[GitHub]</a></span>
					</center>
				</td>
				<!-- <td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href=''> [Talk]</a></span>
		  		  		</center>
		  		  	  </td> -->
				<!-- <td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href=''> [Slides]</a></span>
		  		  		</center>
		  		  	  </td> -->
				<td align=center width=150px>
					<center>
						<span style="font-size:24px"><a
								href='https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf'>
								[Paper]</a></span>
					</center>
				</td>

				<!-- <td align=center width=150px>
	  					<center>
	  						<span style="font-size:24px"><a href=''> [Poster]</a></span>
		  		  		</center>
		  		  	  </td> -->

			</tr>
			<tr>
		</table>
	</center>

	<!--   		  <br><br>
		  <hr> -->

	<br>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/rebasin.png"><img class="rounded" src="./images/rebasin.png" height="500px"></img>
						</href></a><br>
				</center>
			</td>
		</tr>
		<td width=400px>
			<br>
			<center>
				<span style="font-size:14px"><i>(a) The loss landscape for the polynomial approximation task. θA and θB
						are solutions found by SGD. LMC suggests that permuting hidden units of θB would result in πP
						(θB) which is functionally equivalent to before permutation, with no barrier on its linear
						interpolation with θA. (b) Comparison of the cost value in the linear path before (naive) and
						after re-basin. The dashed line in both figures corresponds with the original (naive) path
						between models, and the solid line represents the path and corresponding loss after the proposed
						Sinkhorn re-basin.</i>
			</center>
		</td>

	</table>

	</div>
	<br><br>
	<hr>


	<table align=center width=850px>
		<center>
			<h1>Abstract</h1>
		</center>
	</table>
	The recent emergence of new algorithms for permuting models into functionally equivalent regions of the solution
	space has shed some light on the complexity of error surfaces, and some promising properties like mode connectivity.
	However, finding the right permutation is challenging, and current optimization techniques are not differentiable,
	which makes it difficult to integrate into a gradient-based optimization, and often leads to sub-optimal solutions.
	In this paper, we propose a Sinkhorn re-basin network with the ability to obtain the transportation plan that better
	suits a given objective. Unlike the current state-of-art, our method is differentiable and, therefore, easy to adapt
	to any task within the deep learning domain. Furthermore, we show the advantage of our re-basin method by proposing
	a new cost function that allows performing incremental learning by exploiting the linear mode connectivity property.
	The benefit of our method is compared against similar approaches from the literature, under several conditions for
	both optimal transport finding and linear mode connectivity. The effectiveness of our continual learning method
	based on re-basin is also shown for several common benchmark datasets, providing experimental results that are
	competitive with state-of-art results from the literature.
	<br><br>
	<hr>


	<center>
		<h1>Try our code</h1>
	</center>
	<table align=center width=1000px>
		<tr>
			<td align=center width=750px>
				<center>
			<td><a href='https://github.com/fagp/sinkhorn-rebasin'><img class="round" style="height:450px"
						src="./images/github_example.png" /></a></td>
			</center>
		</tr>
	</table>


	<!-- <table align=center width=800px>
			  <tr><center> <br>
				<span style="font-size:28px">&nbsp;<a href='https://github.com/fagp/sinkhorn-rebasin'>[GitHub]</a> Demo </span></span><i></i>
				<span style="font-size:28px"></a></span>
			  <br>
			  </center></tr>
		  </table> -->


	<br>
	<hr>


	<table align=center width=425px>
		<center>
			<h1>Paper and Supplementary Material</h1>
		</center>
		<tr>

			<td><a
					href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pena_Re-Basin_via_Implicit_Sinkhorn_Differentiation_CVPR_2023_paper.pdf"><img
						class="layered-paper-big" style="height:175px" src="./images/paper_pdf_thumb.png" /></a></td>
			<td><span style="font-size:14pt">
					Fidel A. G. Pena, Heitor Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric Granger, Marco
					Pedersoli.
					<br><br>
					Re-basin via implicit Sinkhorn differentiation.<br>
					In CVPR, 2023.<br><br>
					(hosted on <a href="https://arxiv.org/abs/2212.12042">arXiv</a>)</a>
			</td>
			</td>
		</tr>
	</table>
	<br>


	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt">
					<center>
						<a href="./bibtex.txt">[Bibtex]</a>
					</center>
			</td>
		</tr>
	</table>

	<hr>

	<a name="bw_legacy"></a>
	<center>
		<h1>Results on Different Tasks</h1>
	</center>

	<left>
		<h3>- Model Alignment</h3>
	</left>

	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/toy_example.png"><img class="rounded" src="./images/toy_example.png"
							height="250px"></img></href></a><br>
				</center>
			</td>
		</tr>
		<br>
		<td width=400px>
			<br>
			<center>
				<span style="font-size:14px"><i> Estimated permutation matrices via Weight Matching (WM) and the
						proposed Sinkhorn re-basin. Pi refers to the expected 10 × 10 permutation matrix with ones
						represented in black and zeros in white. The estimated permutations matrix Pˆi shows matching
						permutations as blue squares and miss-matchings in red and yellow. The permutation matrices Pi ∈
						R
						10×10 correspond with transportation plans of layer i, with each layer containing 10 neurons.
						These matrices correspond with actual permutation matrices from the experiment with random
						initialization and 2 hidden layers.</i>
			</center>
		</td>
	</table>
	<br><br><br><br>

	<left>
		<h3>- Linear Mode Connectivity (LMC)</h3>
	</left>
	<table align=center width=850px>
		<tr>
			<td width=400px>
				<center>
					<a href="./images/lmc.png"><img class="rounded" src="./images/lmc.png" height="500px"></img></href>
						</a><br>
				</center>
			</td>
		</tr>
		<td width=400px>
			<br>
			<center>
				<span style="font-size:14px"><i> Example of linear mode connectivity achieved by WM, STE, and our
						Sinkhorn re-basin with CL2, CMid, and CRnd costs for a NN with two hidden layers. Accuracy and
						loss are shown for the Mnist and Cifar10 classification, while only the L2 loss is shown for the
						regression tasks. For Mnist, we include an amplified version of the loss and accuracy for better
						comparison. </i>
			</center>
		</td>
	</table>

	<br><br><br><br>

	<br>
	<hr><br>

	<a name="related_work"></a>
	<table align=center width=1100px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Related Works</h1>
					</center>

					<br>

					Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. <b>Git Re-Basin: Merging Models
						modulo Permutation Symmetries. </b> arXiv preprint arXiv:2209.04836, 2022. <br><br>
					Aditya Kumar Akash, Sixu Li, and Nicolas Garcıa Trillos. <b>Wasserstein Barycenter-based Model
						Fusion and Linear Mode Connectivity of Neural Networks.</b> arXiv preprint arXiv:2210.06671,
					2022. <br><br>
					Frederik Benzing, Simon Schug, Robert Meier, Johannes von Oswald, Yassir Akram, Nicolas Zucchet,
					Laurence Aitchison, and Angelika Steger. <b>Random initialisations performing above chance and how
						to find them. </b> arXiv preprint arXiv:2209.07509, 2022. <br><br>
					Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. <b> The Role of Permutation
						Invariance in Linear Mode Connectivity of Neural Networks. </b> In International Conference on
					Learning Representations, 2022. <br><br>

			</td>
		</tr>
	</table>

	<br>
	<hr>
	<br>

	<table align=center width=1100px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Acknowledgements</h1>
					</center>

					This work was supported by Distech Controls Inc., the Natural Sciences and Engineering Research
					Council of Canada, and the Digital Research Alliance of Canada.
				</left>
			</td>
		</tr>
	</table>


	<br>
	<hr>
	<br>

	<table align=center width=850px>
		<br>
		<tr>
			<td width=400px>
				<center>
					<a href="https://www.etsmtl.ca/"><img class="rounded" src="./images/ets.png" height="100px"></img>
						</href></a><br>
				</center>
			</td>
			<td width=400px>
				<center>
					<a href="https://liviamtl.ca/"><img class="rounded" src="./images/livia.png" height="100px"></img>
						</href></a><br>
				</center>
			</td>
		</tr>



	</table>

	<br><br>

	<script>
		(function (i, s, o, g, r, a, m) {
			i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
				(i[r].q = i[r].q || []).push(arguments)
			}, i[r].l = 1 * new Date(); a = s.createElement(o),
				m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
		})(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

		ga('create', 'UA-75863369-1', 'auto');
		ga('send', 'pageview');

	</script>

</body>

</html>